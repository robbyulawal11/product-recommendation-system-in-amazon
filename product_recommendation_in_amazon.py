# -*- coding: utf-8 -*-
"""Product_Recommendation_In_Amazon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wrDhy3yZcA-KsNnaHWEXFVZsMQBwCWCo

# Product Recommendation System

---
### My Identity
- Name : Robiul Awal
- ID Dicoding : robiul_awal
- Domicile : Yogyakarta
- Email : robbyulawal11@gmail.com
- Linkedin : https://www.linkedin.com/in/robiul-awal11/

---
<h1><center> Product Recommendation System in Amazon Commerce </h1></center>

### Introduction
<p align = "justify"> Amazon is one of the largest and most well-known e-commerce platforms in the world, offering a wide range of products from books, electronics, clothing, to daily necessities. As a global e-commerce company, Amazon has millions of products available to customers in various countries. This very wide product diversity benefits consumers because they have many choices, but also creates challenges for Amazon to help customers find the most relevant products according to their needs and preferences. In this context, the recommendation system becomes very important as a tool to improve the customer shopping experience and optimize sales.

The main problem faced by e-commerce like Amazon is how to present relevant and interesting products to users efficiently. With the large number of products and variations in categories, customers often find it difficult to find the right product amidst the sea of ​​choices available. In addition, the problem of "information overload" or being flooded with information also often arises, where too many choices can make it difficult for customers to make decisions. Another challenge is the "cold start" or the difficulty of providing accurate recommendations for new products or new users who have not had much interaction on the platform. These challenges require a system that can filter, prioritize, and present the most relevant products to each user.

Recommender systems have emerged as an effective solution to address these issues by providing personalized product suggestions based on user data, such as search history, purchases, and product ratings. Amazon implements several approaches in its recommendation system, including content-based and collaborative recommendations. The content-based method uses product attribute data and user preferences to match products to the user’s interests. Meanwhile, the collaborative method leverages behavioral patterns from other users with similar preferences to suggest products that might be of interest. By combining these two approaches, Amazon can provide more accurate and tailored recommendations to the user’s needs.

### Objective
<p align = "justify"> Generate a number of personalized product recommendations for users with content-based filtering techniques.
<p align = "justify"> Generate a number of product recommendations that match user preferences and have never been purchased before with collaborative filtering techniques.

### Dataset
<p align = "justify"> This first dataset consists of an extensive collection of over 2 million customer reviews and ratings for beauty-related products available on the Amazon platform. The dataset includes key information such as:

- Unique User ID for customer identification.
- Product ASIN (Amazon’s unique product identifier).
- Rating, which reflects customer satisfaction on a scale of 1 to 5.
- Timestamp, recorded in UNIX time, indicating when the rating was submitted.

This dataset is just a small part of Amazon’s vast product dataset, which includes 142.8 million reviews spanning the period from May 1996 to July 2014. The full dataset provides a wealth of information, including detailed product reviews, metadata, category information, pricing data, brand details, and even image features.

With the second dataset, you can get an in-depth look at which products are selling the best, which SEO titles are generating the most sales, the best price range for a product in a particular category, and much more.

This dataset consists of two datasets, amazon_products.csv and amazon_categories.csv. These two datasets can be linked through a foreign key relationship where the ‘category_id’ column in ‘amazon_products’ refers to the ‘id’ column in ‘amazon_categories’, allowing us to link each product to the appropriate category.

The large dataset contains 1.4 million Amazon products. Including titles, number of reviews, ratings, prices, and sales data from September 2023.

### Process
- <p align = "justify"> Data Collection: Data collection was done by downloading the dataset from the official reference source from Kaggle, namely at the following link https://www.kaggle.com/datasets/ahmedaliraja/customer-rating-data-by-amazon dan https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products
- <p align = "justify"> Data Understanding: Data Understanding is the initial stage of a project to understand the data we have. In this case, we have 3 separate files regarding ratings, products, and categories
- <p align = "justify"> Univariate Exploratory Data Analysis: At this stage, analysis and exploration of each variable in the data are carried out. In addition, further exploration is also carried out regarding the relationship between one variable and another.
- <p align = "justify"> Data Preprocessing: This is the data preparation stage before the data is used for the next process. At this stage, several files are merged so that they become one complete file unit and are ready to be used in the modeling stage.
- <p align = "justify"> Data Preparation: At this stage, data preparation is carried out and several techniques are carried out such as overcoming missing values. In the content-based recommendation system (content-based filtering) that was developed, one product represents one product category.
- <p align = "justify"> Model Development with Content Based Filtering: At this stage, a recommendation system is developed with the content-based filtering technique. The content-based filtering technique will recommend items that are similar to items that users have liked in the past. At this stage, important feature representations of each product category are found with the tfidf vectorizer and the similarity level is calculated with cosine similarity. After that, a number of product recommendations are made for customers based on previously calculated similarities.
- <p align = "justify">Model Development with Collaborative Filtering: At this stage, the system recommends a number of products based on previously given ratings. From user rating data, we will identify similar products that have never been visited by users to recommend.

References
<p align = "justify"> Dwivedi, Rohit & Anand, Abhineet & Johri, Prashant & Banerji, Arpit & Gaur, N. (2020). Product Based Recommendation System On Amazon Data.
<p align = "justify"> Ahmed, Md Zaid & Singh, Abhay & Paul, Abir & Ghosh, Sayantani & Chaudhuri, Avijit. (2022). Amazon Product Recommendation System. IJARCCE. 11. 10.17148/IJARCCE.2022.11356.

## 1. Importing Packages

---
    
| ⚡ Description: Importing Packages ⚡ |
| :--------------------------- |
| In this section the required packages are imported, and briefly discuss, the libraries that will be used throughout the analysis and modelling. |
"""

# Import library for data preparation
import pandas as pd
import numpy as np
from zipfile import ZipFile
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Import library for modelling with content based filtering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Import library for modelling with collaborative filtering
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""## 2. Loading Data

---
    
| ⚡ Description: Loading the data ⚡ |
| :--------------------------- |
| In this section you are required to load data from kaggle and then extract it to the storage directory. |

---
"""

!rm -rf amazon_dataset/

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d asaniczka/amazon-products-dataset-2023-1-4m-products
!unzip amazon-products-dataset-2023-1-4m-products.zip

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d ahmedaliraja/customer-rating-data-by-amazon
!unzip customer-rating-data-by-amazon.zip

""">The data downloaded from Kaggle has two sources, namely customer rating data from one source and another source provides two datasets, namely products and categories data. The process of modeling recommendation systems requires these three datasets and cannot use only one dataset.

## 3. Exploratory Data Analysis (EDA)

---
    
| ⚡ Description: Exploratory Data Analysis ⚡ |
| :--------------------------- |
| These preprocessing steps aim to remove noise, convert text to a consistent format, and extract important features for further analysis. |

---
"""

amazon_categories = pd.read_csv('/content/amazon_categories.csv')
amazon_products = pd.read_csv('/content/amazon_products.csv')
customer_ratings = pd.read_csv('/content/customer_ratings_data.csv')

"""### Univariate Analysis
Unvariate analysis is performed to determine the distribution of each variable in the dataset. This process produces a visualization that illustrates the comparison of distributions on each variable in the dataset.

#### Amazon Categories Dataset
"""

amazon_categories.info()

"""> Based on the output above, it can be seen that there are 2 variables in amazon_categories. The id variable has an int 64 data type while the category_name variable has an object or character type."""

print('Category Products: ', amazon_categories.category_name.unique())

"""> Based on the output above, you can see 248 categories in the amazon_categories dataset"""

amazon_categories.head()

"""#### Amazon Products Dataset"""

amazon_products.info()

""">- Based on the output above, it can be seen that there are 4 variables of the object data type.
- There are 3 variables of the float64 numeric data type.
- There are 3 variables of the int64 numeric data type.
- And there are a variables of the bool data type.
"""

amazon_products.describe()

"""> Based on the output above, it can be seen that the minimum number of stars a user can give to a product is 0 and the maximum is 5."""

amazon_products.head()

feature = 'isBestSeller'
count = amazon_products[feature].value_counts()
percent = 100*amazon_products[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""> It is known that only around 0.6 percent of products are best sellers."""

amazon_products.hist(bins=50, figsize=(20,15))
plt.show()

"""> From the plot above, we can obtain information that the number of products with a rating of around 4.5 is the largest.

#### Amazon Customer Ratings Dataset
"""

print(customer_ratings.shape)

"""Based on the output above, it can be seen that there are 1048575 records related to rating products sold on Amazon."""

customer_ratings.info()

""">- Based on the output above, it can be seen that there are 2 variables of the object data type and there are 2 variables of the int64 numeric data type."""

customer_ratings.describe()

""">Based on the output above, it can be seen that the minimum rating given to a product is 1 and the maximum rating given is 5."""

customer_ratings.hist(bins=50, figsize=(20,15))
plt.show()

"""> Based on the output above, it can be stated that the rating value most frequently given by customers is 5 and the least is 2."""

print('Number of product category data on Amazon: ', len(amazon_categories.id.unique()))
print('Number of product data: ', len(amazon_products.asin.unique()))
print('Number of userIDs: ', len(customer_ratings["User Id"].unique()))
print('Number of product data assessed: ', len(customer_ratings["Product Id"].unique()))
print('Total data of all assessments: ', len(customer_ratings))

"""### Multivariate Analysis
Multivariate analysis is used to determine the correlation between features and targets, namely the prediction of passenger safety in the Titanic ship accident. In this process, temporary assumptions can be obtained regarding what characteristics have a higher life expectancy in the Titanic ship accident.

#### Data Preprocessing
This is the data preparation stage before the data is used for the next process. At this stage, several files are merged so that they become one complete file unit and are ready to be used in the modeling stage.

>The following is a change in the name of the id column in amazon_categories to category_id so that it is the same as the column name in amazon_products.
"""

amazon_categories= amazon_categories.rename(columns={'id': 'category_id'})

"""> The following is a merge of amazon_products with amzon_categories based on category_id."""

# Menggabungkan dataframe product dengan category berdasarkan nilai category_id
product_info = pd.merge(amazon_products, amazon_categories , on='category_id', how='left')

product_info.info()

"""> handling missing value in product_info dataset"""

# Check for missing values ​​with the isnull() function
product_info.isnull().sum()

# Membersihkan missing value dengan fungsi dropna()
clean_product_info = product_info.dropna()
clean_product_info.info()

# Menghitung jumlah rating, product_rating, dan service kemudian menggabungkannya berdasarkan category_id
clean_product_info.groupby('category_id').sum()

# Definisikan dataframe rating ke dalam variabel all_product_rate
all_product_rate = customer_ratings
all_product_rate

""">The following is a change in the name of the Product Id column in clean_product_info to product_id."""

clean_product_info = clean_product_info.rename(columns = {'asin':'product_id', 'title':'product_name'})

clean_product_info.head()

""">The following is a change in the name of the Product Id column in all_product_rate to product_id so that it is the same as the column name in amazon_products."""

all_product_rate = all_product_rate.rename(columns = {'Product Id':'product_id'})

all_product_rate.info()

common_product_ids = all_product_rate['product_id'].isin(clean_product_info['product_id'])
print(common_product_ids.any())

"""> The following is a merge of clean_product_info with all_product_rate based on product_id."""

# Menggabungkan data hanya untuk 'product_id' yang cocok
final_data = pd.merge(
    all_product_rate,
    clean_product_info,
    on='product_id',
    how='left'
)

# Menggabungkan data hanya untuk 'product_id' yang cocok
all_product = pd.merge(
    all_product_rate,
    clean_product_info[['product_id', 'product_name', 'category_id', 'category_name']],
    on='product_id',
    how='left'
)

# Menampilkan DataFrame yang sudah digabungkan
all_product

"""#### Data Visualization"""

sns.pairplot(final_data, diag_kind = 'kde')

"""> From the image above, it can be seen that each variable has a random correlation with other variables, except for the price variable with the list price which shows a slight positive correlation between the two variables."""

numerical_features_product = ['Rating', 'Time stamp', 'stars', 'reviews', 'price', 'listPrice', 'category_id', 'isBestSeller', 'boughtInLastMonth']

plt.figure(figsize=(10, 8))
correlation_matrix = final_data[numerical_features_product].corr().round(2)

# To print the value in the box, use the not=True parameter
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix", size=20)

"""> In line with the results of the correlation of each numeric variable above, the highest correlation is found in the correlation between price and list price of 0.43. However, other variables appear to have very weak correlations, even most of them have correlation values ​​close to 0.

# Content Based Filtering

## 4. Data Preparation

---
    
| ⚡ Description: Data preparation ⚡ |
| :--------------------------- |
| These preparation steps aim handling missing values, handling duplicate values, and feature encoding. |

---

### Handling Missing Values
>After analyzing the variables, it is necessary to clean the dataset so that it can be processed properly. In this project, cleaning will be carried out on several variables. The first will be the deletion of rows that have empty data on the product_name, category_id, and category_name variables.
"""

# Mengecek missing value pada dataframe all_product
all_product.isnull().sum()

# Membersihkan missing value dengan fungsi dropna()
all_product_clean = all_product.dropna()
all_product_clean

# Mengecek kembali missing value pada variabel all_product_clean
all_product_clean.isnull().sum()

# Mengurutkan product berdasarkan PlaceID kemudian memasukkannya ke dalam variabel fix_product
fix_product = all_product_clean.sort_values('product_id', ascending=True)
fix_product

# Mengecek berapa jumlah fix_product
len(fix_product.product_id.unique())

# Mengecek kategori produk yang unik
fix_product.category_name.unique()

# Membuat variabel preparation yang berisi dataframe fix_product kemudian mengurutkan berdasarkan placeID
preparation = fix_product
preparation.sort_values('product_id')

"""### Handling Duplicates Values
In the content based filtering technique, the data used is unique product ID data with category name. So that the required record is only one, so that duplicate rating records that have the same product ID will be deleted.
"""

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('product_id')
preparation

"""### Feature Selection
Selecting the variables used and creating a new dataframe and not taking unused features
"""

# Mengonversi data series ‘product_id’ menjadi dalam bentuk list
product_id = preparation['product_id'].tolist()

# Mengonversi data series ‘Name’ menjadi dalam bentuk list
product_name = preparation['product_name'].tolist()

# Mengonversi data series ‘Rcategory’ menjadi dalam bentuk list
product_category = preparation['category_name'].tolist()

print(len(product_id))
print(len(product_name))
print(len(product_category))

"""> There were 1482 unique products successfully recorded"""

# Membuat dictionary untuk data ‘product_id’, ‘product_name’, dan ‘category’
product_new = pd.DataFrame({
    'product_id': product_id,
    'product_name': product_name,
    'category_name': product_category
})
product_new

print('Category Products: ', product_new.category_name.unique())

data = product_new
data.sample(5)

"""### Feature Extraction
TF-IDF (Term Frequency-Inverse Document Frequency) is a technique for extracting features from text data, mainly used in text mining. This technique aims to determine how important a word or term is in a document relative to a collection of documents (corpus). Therefore, this technique will also be used in recommendation systems to find important feature representations of each product category.
"""

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data cuisine
tf.fit(data['category_name'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['category_name'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""> Note that the matrix we have is of size (95, 22). The value 95 is the data size and 22 is the product category matrix."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis masakan
# Baris diisi dengan nama product

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.product_name
).sample(22, axis=1).sample(10, axis=0)

"""## 5. Modelling

---
    
| ⚡ Description: Modelling ⚡ |
| :--------------------------- |
| In this section, we will carry out training data using the cosine similarity model that has been created. |

---
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama resto
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['product_name'], columns=data['product_name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap product
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""## 6. Prediction

---
    
| ⚡ Description: Prediction ⚡ |
| :--------------------------- |
|In this section, it generates a number of products to be recommended to users.|

---

> Here, we create a product_recommendations function with several parameters as follows:

- Product_name: Product name (dataframe similarity index).
- Similarity_data: Dataframe about previously defined similarity.
- Items: Name and features used to define similarity, in this case ‘product_name’ and ‘category_name’.
- k: Number of recommendations to be given.
"""

def product_recommendations(nama_product, similarity_data=cosine_sim_df, items=data[['product_name', 'category_name']], k=5):
    """
    Rekomendasi produk berdasarkan kemiripan dataframe, dengan menghitung precision.

    Parameter:
    ---
    nama_product : str
        Nama produk (index kemiripan dataframe).
    similarity_data : pd.DataFrame
        Kesamaan dataframe, simetrik, dengan produk sebagai indeks dan kolom.
    items : pd.DataFrame
        Mengandung nama produk dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan.
    k : int
        Jumlah rekomendasi yang diberikan.

    Returns:
    ---
    tuple : (pd.DataFrame, float)
        DataFrame dengan rekomendasi produk dan nilai precision.
    """

    # Ambil kategori dari produk input
    category_input = items.loc[items['product_name'] == nama_product, 'category_name'].values[0]

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    index = similarity_data.loc[:, nama_product].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_product agar nama produk yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_product, errors='ignore')

    # Mendapatkan DataFrame hasil rekomendasi
    recommendations = pd.DataFrame(closest, columns=['product_name']).merge(items, on='product_name').head(k)

    # Menghitung precision berdasarkan kesamaan kategori
    recommendations['is_relevant'] = recommendations['category_name'] == category_input
    precision = recommendations['is_relevant'].mean()  # Persentase rekomendasi yang relevan

    # Mengembalikan DataFrame rekomendasi dan nilai precision
    return recommendations, precision

data[data.product_name.eq('jane iredale PureMatte Finishing Powder Refill')]

# Memanggil fungsi rekomendasi
nama_produk = "jane iredale PureMatte Finishing Powder Refill"
rekomendasi_df, precision_value = product_recommendations(nama_product=nama_produk, k=5)

# Menampilkan hasil
print("DataFrame Rekomendasi:")
print(rekomendasi_df)
print("\nNilai Precision:")
print(precision_value)

"""## 7. Evaluation Model

---
    
| ⚡ Description: Data preprocessing ⚡ |
| :--------------------------- |
| In evaluating a recommendation system that uses the cosine_similarity technique, one of the evaluation metrics that can be used to measure how well the system provides recommendations to users is precision. Precision measures the proportion of recommended items that are relevant. In the context of a recommendation system, precision is calculated as the ratio of the number of truly relevant recommendations to the total number of recommended items.|

---
"""

data[data.product_name.eq('100 Derby Professional Single Edge Razor Blades')]

# Memanggil fungsi rekomendasi
nama_produk = "100 Derby Professional Single Edge Razor Blades"
rekomendasi_df, precision_value = product_recommendations(nama_product=nama_produk, k=5)

# Menampilkan hasil
print("DataFrame Rekomendasi:")
print(rekomendasi_df)
print("\nNilai Precision:")
print(precision_value)

"""> It can be seen from the recommendation results above, all the recommendation results provide products with the inputted category. It is proven that the evaluation results are the same as 1, which means it shows 100% level of truth.

# Collaborative Filtering

## 4. Data Preparation

---
    
| ⚡ Description: Data preparation ⚡ |
| :--------------------------- |
| These preparation steps aim encoding caracter or object data, and splitting data. |

---
"""

# Membaca dataset
df = fix_product
df

"""### Feature Encoding
To perform the process of coding category features, it can be done manually as follows. In this project, there are two category variables, namely 'user_id' and 'product_id'. This coding process is carried out so that variables whose values ​​are objects can become numeric values ​​so that they can be processed.
"""

# Mengubah User Id menjadi list tanpa nilai yang sama
user_ids = df['User Id'].unique().tolist()

# Melakukan encoding User Id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke ke User Id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah Product Id menjadi list tanpa nilai yang sama
product_ids = df.product_id.unique().tolist()

# Melakukan proses encoding Product Id
product_to_product_encoded = {x: i for i, x in enumerate(product_ids)}

# Melakukan proses encoding angka ke Product Id
product_encoded_to_product = {i: x for i, x in enumerate(product_ids)}

# Mapping userID ke dataframe user
df['user'] = df['User Id'].map(user_to_user_encoded)

# Mapping product Id ke dataframe product
df['product'] = df.product_id.map(product_to_product_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah product
num_product = len(product_encoded_to_product)
print(num_product)

# Mengubah rating menjadi nilai float
df['Rating'] = df['Rating'].values.astype(np.float32)

# Nilai minimum Rating
min_Rating = min(df['Rating'])

# Nilai maksimal Rating
max_Rating = max(df['Rating'])

print('Number of User: {}, Number of product: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_product, min_Rating, max_Rating
))

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""### Feature Selection and Normalization
Selecting the variables used and creating a new data frame and not taking unused features. In addition, normalization is also carried out on the rating feature.
"""

# Membuat variabel x untuk mencocokkan data user dan product menjadi satu value
x = df[['user', 'product']].values

# Membuat variabel y untuk membuat Rating dari hasil
y = df['Rating'].apply(lambda x: (x - min_Rating) / (max_Rating - min_Rating)).values

"""### Splitting Data
This separation is important in machine learning to evaluate the performance of a model using data that the model never saw during training, thus providing a more realistic picture of how the model performs on unseen data. This data separation is also used in manual processes such as the following
"""

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## 5. Training Model

---
    
| ⚡ Description: Training model ⚡ |
| :--------------------------- |
| In this section, we will carry out training data using the RecommenderNet model that has been created. |

---
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_product, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_product = num_product
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.product_embedding = layers.Embedding( # layer embeddings product
        num_product,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.product_bias = layers.Embedding(num_product, 1) # layer embedding product bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    product_vector = self.product_embedding(inputs[:, 1]) # memanggil layer embedding 3
    product_bias = self.product_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_product = tf.tensordot(user_vector, product_vector, 2)

    x = dot_user_product + user_bias + product_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_product, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
with tf.device('/device:GPU:0'):
  history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 10,
    validation_data = (x_val, y_val)
  )

"""## 6. Evaluation Model

---
    
| ⚡ Description: Data preprocessing ⚡ |
| :--------------------------- |
| Root Mean Squared Error (RMSE) is an evaluation metric often used in recommendation systems to measure how well a model predicts the actual data. RMSE measures the average of the squared differences between the model's predicted values ​​and the actual values, then squared. This metric gives an idea of ​​how far the prediction is from the actual value in the same scale unit as the original data.|

---
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Train and Test Decrease: Early in training (epochs 0-2), the RMSE values ​​for both the train and test data decrease rapidly, indicating that the model is quickly learning patterns in the data. The small gap indicates that the model is doing quite well at generalizing, but a small gap remains, which could indicate minor overfitting. This graph shows that the recommendation system model experiences an increase in accuracy as the number of epochs increases, both on the train data which managed to reach an RMSE value of 0.2633 and the test which managed to reach an RMSE value of 0.3045.

## 7. Prediction

---
    
| ⚡ Description: Prediction ⚡ |
| :--------------------------- |
|In this section, it generates a number of products to be recommended to users.|

---
"""

product_df = product_new
df = fix_product

# Mengambil sample user
user_id = df["User Id"].sample(43545).iloc[0]
product_visited_by_user = df[df["User Id"] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
product_not_visited = product_df[~product_df['product_id'].isin(product_visited_by_user.product_id.values)]['product_id']
product_not_visited = list(
    set(product_not_visited)
    .intersection(set(product_to_product_encoded.keys()))
)

product_not_visited = [[product_to_product_encoded.get(x)] for x in product_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_product_array = np.hstack(
    ([[user_encoder]] * len(product_not_visited), product_not_visited)
)

"""> To get product recommendations, we first take a random sample of users and define a variable product_not_bought which is a list of products that have never been visited by the user.

> The variable peoduct_not_visited is obtained by using the bitwise operator (~) on the variable product_bought_by_user.
"""

ratings = model.predict(user_product_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_product_ids = [
    product_encoded_to_product.get(product_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('product with high ratings from user')
print('----' * 8)

top_product_user = (
    product_visited_by_user.sort_values(
        by = 'Rating',
        ascending=False
    )
    .head(5)
    .product_id.values
)

product_df_rows = product_df[product_df['product_id'].isin(top_product_user)]
for row in product_df_rows.itertuples():
    print(row.product_name, ':', row.category_name)

print('----' * 8)
print('Top 10 product recommendation')
print('----' * 8)

recommended_product = product_df[product_df['product_id'].isin(recommended_product_ids)]
for row in recommended_product.itertuples():
    print(row.product_name, ':', row.category_name)

"""> Based on the output above, it can be seen that the recommendation results are quite good. Users give high ratings to the skin care and Personal Care product categories. Then, the recommendations given by the system are products in the skin care, Foot, Hand & Nail Care, Beauty Tools & Accessories, Hair Care, Health Care Products categories. All recommended products are still very related to the products given the highest rating by users.

# Save Model
In this section, the process of storing the model is carried out so that it can be applied to real projects such as applications
"""

model.save("model.h5")

# Menyimpan model dalam format SavedModel
export_dir = 'saved_model/'
tf.saved_model.save(model, export_dir)

# Convert model.h5 to model
!tensorflowjs_converter --input_format=keras model.h5 tfjs_model

"""# Export requirements.txt
This section will export the libraries used in this project in the form of a .txt file.
"""

pip freeze > requirements.txt